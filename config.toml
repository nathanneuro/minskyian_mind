[run]
max_steps = 200
prompt = "What is the most promising approach to measuring consciousness in AI systems?"

# What becomes the "accepted truth" routed between rooms:
#   "raw"      - raw LLM output (skip T5 even if loaded)
#   "edited"   - T5-edited output (default)
#   "improved" - judge counterfactual replaces routed messages each cycle
room_output_source = "edited"

# Max characters per between-room message (~256 chars â‰ˆ 64 tokens).
# External messages, tool calls/outputs are always unbounded.
max_message_length = 256

[run.features]
llm = true
t5 = true
summarizers = true
judges = true
forecasts = true
fake_user = true

[run.intervals]
summarizer = 10
judge = 1

[llm]
backend = "hf"
model_name = "Qwen/Qwen3-8B"
device = "cuda:0"
dtype = "float16"
max_tokens = 256
temperature = 0.7
top_p = 0.9
use_chat_template = true

# To switch to RWKV, replace [llm] above with:
# [llm]
# backend = "rwkv"
# model_name = "data/models/rwkv7-g1d-7.2b-20260131-ctx8192"
# device = "cuda:0"
# dtype = "float16"
# max_tokens = 256
# temperature = 1.0
# top_p = 0.5
# strategy = "cuda fp16"
# state_file = "rwkv_state.pt"
# use_chat_template = false

# To use an API model (no local GPU needed for the frozen LLM):
# [llm]
# backend = "api"
# model_name = "gpt-4o"          # or "deepseek-chat", "claude-sonnet-4-5-20250929", etc.
# max_tokens = 256
# temperature = 0.7
# top_p = 0.9
# api_base_url = ""              # leave empty for OpenAI, or set for other providers
# api_key_env = "OPENAI_API_KEY" # env var name holding the API key

[agents]
model = "QuantTrio/DeepSeek-V3.2-AWQ"
base_url = "https://api.infinity.inc/v1"
api_key_env = "INF_API_KEY"
max_tokens = 1000
temperature = 0.7

[t5]
device = "cuda:1"
max_input_length = 512
max_output_length = 512

[training]
learning_rate = 1e-4
batch_size = 4
epochs = 1
